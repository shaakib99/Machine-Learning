{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression with gradient Descent.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPS9A3zMocwUmEu/uh7wr/P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaakib99/Machine-Learning/blob/gcolab/Linear_Regression_with_gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdH-RjnM7-rY",
        "colab_type": "text"
      },
      "source": [
        "***Import Necessary Libraries***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-zdkHaC7FEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0UCl06O8I3n",
        "colab_type": "text"
      },
      "source": [
        "**Load Dataset** <br>\n",
        "I will take 50% of the data as training dataset.\n",
        "Others as test Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89BU99Lb7kJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('./Datasets/Student_stat.csv')\n",
        "\n",
        "data_split_percentage = 50 # %\n",
        "\n",
        "'''\n",
        "  Check whether data_split_percentage is over 100% adn is not below 0\n",
        "'''\n",
        "if data_split_percentage <=100 and data_split_percentage > 0:\n",
        "  train = data.sample(frac=data_split_percentage/100)\n",
        "  test = data.drop(train.index)\n",
        "  train_data = list(train['SAT'])\n",
        "  train_label = list(train['GPA'])\n",
        "  test_data = list(test['SAT'])\n",
        "  test_label = list(test['GPA'])\n",
        "else:\n",
        "  print(\"data_split_percentage can not go over 100 and can not go below 0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk4BQTmg_mVS",
        "colab_type": "text"
      },
      "source": [
        "**Dataset Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DeSO5Nj_q5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(test_data,test_label, color='red')\n",
        "plt.scatter(train_data,train_label, color='blue')\n",
        "plt.legend(['TEST','TRAIN'])\n",
        "plt.xlabel('SAT')\n",
        "plt.ylabel('GPA')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eba5bIMVLlL0",
        "colab_type": "text"
      },
      "source": [
        "Let's Write Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPljpBGILpg-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a6eeb96-49b9-4e22-f8d7-20d8aaa2fd59"
      },
      "source": [
        "data_length = len(train_data)\n",
        "\n",
        "'''\n",
        "  Our train data is just one-dimensional data.Meaning only one weight\n",
        "'''\n",
        "weight = [1] # Bias node\n",
        "weight.append(0) # only one feature\n",
        "\n",
        "\n",
        "# Hypothesis function\n",
        "def hypothesis(data,weight):\n",
        "  total = 0\n",
        "  # for i in range(len(data)):\n",
        "  total += data*weight[1] + weight[0]\n",
        "  return total\n",
        "\n",
        "\n",
        "# Iteration\n",
        "iteration = 100\n",
        "for i in range(iteration):\n",
        "\n",
        "  square_cost = 0\n",
        "  actual_cost = 0\n",
        "\n",
        "  for j in range(data_length):\n",
        "    hypo = hypothesis(train_data[j], weight)\n",
        "    square_cost += (1/data_length) * (hypo - train_label[j])**2\n",
        "    actual_cost = (1/data_length) * (hypo - train_label[j])\n",
        "    weight[0] -= 0.5*0.01 * actual_cost\n",
        "    weight[1] -= 0.5 * 0.01 * actual_cost * train_label[j]\n",
        "\n",
        "  print(square_cost)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1887154594510577\n",
            "0.0644705912860816\n",
            "0.06446942780201655\n",
            "0.06446826439799791\n",
            "0.0644671010740224\n",
            "0.06446593783008692\n",
            "0.06446477466618818\n",
            "0.06446361158232303\n",
            "0.06446244857848804\n",
            "0.06446128565468023\n",
            "0.06446012281089622\n",
            "0.06445896004713293\n",
            "0.06445779736338704\n",
            "0.06445663475965535\n",
            "0.06445547223593463\n",
            "0.06445430979222164\n",
            "0.06445314742851324\n",
            "0.06445198514480611\n",
            "0.06445082294109714\n",
            "0.06444966081738296\n",
            "0.06444849877366053\n",
            "0.06444733680992648\n",
            "0.06444617492617759\n",
            "0.06444501312241079\n",
            "0.06444385139862273\n",
            "0.06444268975481031\n",
            "0.0644415281909701\n",
            "0.06444036670709911\n",
            "0.06443920530319397\n",
            "0.06443804397925151\n",
            "0.06443688273526857\n",
            "0.06443572157124182\n",
            "0.0644345604871681\n",
            "0.06443339948304423\n",
            "0.06443223855886693\n",
            "0.06443107771463298\n",
            "0.06442991695033916\n",
            "0.06442875626598232\n",
            "0.06442759566155913\n",
            "0.06442643513706653\n",
            "0.06442527469250113\n",
            "0.06442411432785987\n",
            "0.0644229540431394\n",
            "0.06442179383833657\n",
            "0.06442063371344818\n",
            "0.06441947366847095\n",
            "0.06441831370340169\n",
            "0.06441715381823723\n",
            "0.06441599401297428\n",
            "0.06441483428760968\n",
            "0.06441367464214016\n",
            "0.06441251507656254\n",
            "0.06441135559087362\n",
            "0.06441019618507011\n",
            "0.06440903685914888\n",
            "0.06440787761310672\n",
            "0.06440671844694032\n",
            "0.0644055593606465\n",
            "0.06440440035422208\n",
            "0.06440324142766385\n",
            "0.06440208258096848\n",
            "0.06440092381413297\n",
            "0.06439976512715394\n",
            "0.06439860652002817\n",
            "0.0643974479927525\n",
            "0.06439628954532375\n",
            "0.0643951311777386\n",
            "0.06439397288999395\n",
            "0.06439281468208649\n",
            "0.06439165655401305\n",
            "0.06439049850577039\n",
            "0.06438934053735537\n",
            "0.06438818264876474\n",
            "0.06438702483999517\n",
            "0.06438586711104355\n",
            "0.06438470946190673\n",
            "0.06438355189258141\n",
            "0.06438239440306437\n",
            "0.06438123699335245\n",
            "0.06438007966344239\n",
            "0.064378922413331\n",
            "0.06437776524301506\n",
            "0.06437660815249131\n",
            "0.06437545114175663\n",
            "0.06437429421080777\n",
            "0.0643731373596415\n",
            "0.06437198058825465\n",
            "0.06437082389664386\n",
            "0.06436966728480614\n",
            "0.06436851075273814\n",
            "0.06436735430043666\n",
            "0.0643661979278985\n",
            "0.06436504163512052\n",
            "0.06436388542209938\n",
            "0.06436272928883192\n",
            "0.06436157323531498\n",
            "0.06436041726154532\n",
            "0.0643592613675196\n",
            "0.06435810555323482\n",
            "0.0643569498186877\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}